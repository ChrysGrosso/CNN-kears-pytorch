{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0de9b99",
   "metadata": {},
   "source": [
    "## Le deuxième objectif du projet consiste à expliciter et à implémenter 2 approches de transfer learning : \n",
    "**A : l’utilisation des features d’un DL avant la couche dense comme représentation des images puis apprentissage d’un modèle de ML “classique”**\n",
    "**B : fine-tuning d'un modèle existant à de nouvelles données**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092ce89",
   "metadata": {},
   "source": [
    "# Transfer learning de type A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e5e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changement de répertoire\n",
    "import os\n",
    "os.chdir(\"C:/MLDL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df48e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training damage images:  5000\n",
      "total validation damage images:  1000\n",
      "total test damage images:  1000\n",
      "total training no damage images:  5000\n",
      "total validation no damage images:  1000\n",
      "total test no damage images:  1000\n"
     ]
    }
   ],
   "source": [
    "train_damage_dir = \"C:/MLDL/train_another/damage\"\n",
    "validation_damage_dir = \"C:/MLDL/validation_another/damage\"\n",
    "test_damage_dir = \"C:/MLDL/test/damage\"\n",
    "\n",
    "train_nodamage_dir = \"C:/MLDL/train_another/no_damage\"\n",
    "validation_nodamage_dir = \"C:/MLDL/validation_another/no_damage\"\n",
    "test_nodamage_dir = \"C:/MLDL/test/no_damage\"\n",
    "\n",
    "print('total training damage images: ',len(os.listdir(train_damage_dir)))\n",
    "print('total validation damage images: ',len(os.listdir(validation_damage_dir)))\n",
    "print('total test damage images: ',len(os.listdir(test_damage_dir)))\n",
    "print('total training no damage images: ',len(os.listdir(train_nodamage_dir)))\n",
    "print('total validation no damage images: ',len(os.listdir(validation_nodamage_dir)))\n",
    "print('total test no damage images: ',len(os.listdir(test_nodamage_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c44e7",
   "metadata": {},
   "source": [
    "## Avec VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad1d90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chrys\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\chrys\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 150, 150]           1,792\n",
      "              ReLU-2         [-1, 64, 150, 150]               0\n",
      "            Conv2d-3         [-1, 64, 150, 150]          36,928\n",
      "              ReLU-4         [-1, 64, 150, 150]               0\n",
      "         MaxPool2d-5           [-1, 64, 75, 75]               0\n",
      "            Conv2d-6          [-1, 128, 75, 75]          73,856\n",
      "              ReLU-7          [-1, 128, 75, 75]               0\n",
      "            Conv2d-8          [-1, 128, 75, 75]         147,584\n",
      "              ReLU-9          [-1, 128, 75, 75]               0\n",
      "        MaxPool2d-10          [-1, 128, 37, 37]               0\n",
      "           Conv2d-11          [-1, 256, 37, 37]         295,168\n",
      "             ReLU-12          [-1, 256, 37, 37]               0\n",
      "           Conv2d-13          [-1, 256, 37, 37]         590,080\n",
      "             ReLU-14          [-1, 256, 37, 37]               0\n",
      "           Conv2d-15          [-1, 256, 37, 37]         590,080\n",
      "             ReLU-16          [-1, 256, 37, 37]               0\n",
      "        MaxPool2d-17          [-1, 256, 18, 18]               0\n",
      "           Conv2d-18          [-1, 512, 18, 18]       1,180,160\n",
      "             ReLU-19          [-1, 512, 18, 18]               0\n",
      "           Conv2d-20          [-1, 512, 18, 18]       2,359,808\n",
      "             ReLU-21          [-1, 512, 18, 18]               0\n",
      "           Conv2d-22          [-1, 512, 18, 18]       2,359,808\n",
      "             ReLU-23          [-1, 512, 18, 18]               0\n",
      "        MaxPool2d-24            [-1, 512, 9, 9]               0\n",
      "           Conv2d-25            [-1, 512, 9, 9]       2,359,808\n",
      "             ReLU-26            [-1, 512, 9, 9]               0\n",
      "           Conv2d-27            [-1, 512, 9, 9]       2,359,808\n",
      "             ReLU-28            [-1, 512, 9, 9]               0\n",
      "           Conv2d-29            [-1, 512, 9, 9]       2,359,808\n",
      "             ReLU-30            [-1, 512, 9, 9]               0\n",
      "        MaxPool2d-31            [-1, 512, 4, 4]               0\n",
      "================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.26\n",
      "Forward/backward pass size (MB): 96.55\n",
      "Params size (MB): 56.13\n",
      "Estimated Total Size (MB): 152.94\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#TRANSFER LEARNING\n",
    "#The authors did : using feature extraction from ImageNet VGG16 - library : keras\n",
    "#We do : using feature extraction from ImageNet VGG16 - library : pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch.cuda\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "conv_base = models.vgg16(pretrained=True) # pre-trained model\n",
    "conv_base = conv_base.to(device)\n",
    "conv_base = torch.nn.Sequential(*list(conv_base.children())[:-2]) # remove the last layer\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(conv_base, (3, 150, 150)) #same input shape\n",
    "\n",
    "#summary check : ok\n",
    "#Total params: 14,714,688\n",
    "#Trainable params: 14,714,688\n",
    "#Non-trainable params: 0\n",
    "#max Pool 2D : same layer, ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40e002dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 1: Fast feature extraction without data augmentation\n",
    "#Run the conv_base on the dataset and save as Numpy array on disk\n",
    "#Then build the dense layer on this\n",
    "#This is faster to run, but we cannot augment the data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dir = \"C:/MLDL/train_another\"\n",
    "validation_dir = \"C:/MLDL/validation_another\"\n",
    "test_dir = \"C:/MLDL/test\"\n",
    "#transform\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transform)\n",
    "validation_dataset = datasets.ImageFolder(root=validation_dir, transform=data_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=data_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=20, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc696ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define extract_features\n",
    "#generators yield data indefinitely\n",
    "#have to break after we have seen every image once\n",
    "#try parallized (cuda)\n",
    "\n",
    "import torch.cuda\n",
    "\n",
    "def extract_features(dataloader, sample_count):\n",
    "    batch_size = dataloader.batch_size\n",
    "    features = torch.zeros(size=(sample_count, 4, 4, 512), dtype=torch.float32).to(device)\n",
    "    labels = torch.zeros(size=(sample_count,1), dtype=torch.float32).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs_batch, labels_batch) in enumerate(dataloader):\n",
    "            inputs_batch = inputs_batch.to(device)\n",
    "            features_batch = conv_base(inputs_batch).view(inputs_batch.size(0), 4,4,512)\n",
    "            features[i * len(inputs_batch): (i + 1) * len(inputs_batch)] = features_batch\n",
    "            labels_batch = labels_batch.view(-1, 1)\n",
    "            labels[i * len(inputs_batch): (i + 1) * len(inputs_batch)] = labels_batch.to(device)\n",
    "            if i * batch_size >= sample_count:\n",
    "                break\n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "017f0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction\n",
    "# Check if CUDA is available and move the features and labels to the GPU\n",
    "train_features, train_labels = extract_features(train_loader, 10000)\n",
    "if torch.cuda.is_available():\n",
    "    train_features = train_features.cuda()\n",
    "    train_labels = train_labels.cuda()\n",
    "\n",
    "validation_features, validation_labels = extract_features(validation_loader, 2000)\n",
    "if torch.cuda.is_available():\n",
    "    val_features = validation_features.cuda()\n",
    "    validation_labels = validation_labels.cuda()\n",
    "\n",
    "test_features, test_labels = extract_features(test_loader, 2000)\n",
    "if torch.cuda.is_available():\n",
    "    test_features = test_features.cuda()\n",
    "    test_labels = test_labels.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd353a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten steps\n",
    "train_features = train_features.reshape(10000, 512*4*4)\n",
    "validation_features = validation_features.reshape(2000, 512*4*4)\n",
    "test_features = test_features.reshape(2000, 512*4*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70853161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the densely connected layer\n",
    "#import Optimizers\n",
    "import torch.optim as optim\n",
    "\n",
    "model = DenseNet()\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.conv_base = conv_base #to initialize the connection with the pre-trained model (whose dense layers were removed)\n",
    "        self.fc1 = nn.Linear(512*4*4, 8192) #input_size (#256,1 defined per default as per see below)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(8192,20)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_base(x)\n",
    "        x = x.view(x.shape[0], -1) # to reshape the inputs into 1 dimension\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e356e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (conv_base): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=8192, out_features=256, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Check whether customized dense layers are correctly connected to VGG16\n",
    "from torchsummary import summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866d1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [1:12:09<00:00,  8.66s/it]\n",
      "100%|██████████| 500/500 [1:12:22<00:00,  8.68s/it]\n",
      "100%|██████████| 500/500 [1:12:28<00:00,  8.70s/it]\n",
      "100%|██████████| 500/500 [1:12:01<00:00,  8.64s/it]\n",
      "100%|██████████| 500/500 [1:12:28<00:00,  8.70s/it]\n",
      "100%|██████████| 500/500 [1:12:06<00:00,  8.65s/it]\n",
      "100%|██████████| 500/500 [1:12:03<00:00,  8.65s/it]\n",
      "100%|██████████| 500/500 [1:11:47<00:00,  8.62s/it]\n",
      " 47%|████▋     | 234/500 [33:27<39:05,  8.82s/it] "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the loss function and the optimizer, with the same parameters as the scientists implemented\n",
    "criterion = nn.BCELoss() #Binary cross-entrpoy\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=2e-5) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion.to(device) #loss function\n",
    "\n",
    "history = {'loss': [], 'val_loss': [], 'acc': [], 'val_acc': []}\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    for i, data in enumerate(tqdm(train_loader), 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.float().view(-1,1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracy += (torch.round(outputs) == labels.float().view(-1,1)).sum().item() / len(labels)\n",
    "        \n",
    "    # train on 500 *20 batches =10 000 images, validation on 100 *20 batches = 2000 images\n",
    "    if i % 1000 == 999: # every 1000 mini-batches\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            accuracy_val = 0\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            for val_data in validation_loader:\n",
    "                val_inputs, val_labels = val_data\n",
    "                val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels.float().view(-1,1)).item()\n",
    "                accuracy_val += (torch.round(val_outputs) == val_labels.float().view(-1,1)).sum().item() / len(val_labels)\n",
    "            val_loss /= len(validation_loader)\n",
    "            accuracy_val /= len(validation_loader)\n",
    "            history['loss'].append(running_loss / 100)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['acc'].append(accuracy / 100)\n",
    "            history['val_acc'].append(accuracy_val)\n",
    "            running_loss = 0.0\n",
    "            accuracy = 0.0\n",
    "end = time.perf_counter()\n",
    "print('Epoch {}/30, 500/500 [==============================] - loss: {:.4f} - acc: {:.4f} - val_loss: {:.4f} - val_acc: {:.4f}'.format(\n",
    "    epoch + 1, history['loss'][-1], history['acc'][-1], history['val_loss'][-1], history['val_acc'][-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model with pretrained features in Numpy array (no data augmentation)\n",
    "torch.save(model.state_dict(), 'tomnod_transfer_VGG16.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the loss and accuracy\n",
    "\n",
    "\n",
    "plt.plot(epochs, history['acc'], 'bo', label='Training acc')\n",
    "plt.plot(epochs, history['val_acc'], 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy with pre-trained features')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "epochs = range(1, len(history['loss']) + 1)\n",
    "plt.plot(epochs, history['loss'], 'bo', label='Training loss')\n",
    "plt.plot(epochs, history['val_loss'], 'r', label='Validation loss')\n",
    "plt.title('Training and validation accuracy with pre-trained features')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad94f8f",
   "metadata": {},
   "source": [
    "# Transfer learning de type B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pre-trained convolutional base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch.cuda\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "conv_base = models.vgg16(pretrained=True) # pre-trained model\n",
    "conv_base = conv_base.to(device)\n",
    "conv_base = torch.nn.Sequential(*list(conv_base.children())[:-2]) # remove the last layer\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(conv_base, (3, 150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d57a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data augmentation with the same parameters used by the scientists\n",
    "#freeze the paramters of the pre-trained network\n",
    "for param in conv_base.parameters():\n",
    "    param.requires_grad = False # les gradients ne seront pas calculés lors de la phse d'entaînement pour ces paramtères\n",
    "\n",
    "\n",
    "# Add custom fully connected layers\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_base = conv_base\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(25088, 256) #205088, poids de la dernière couche\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_base(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = CustomClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ba30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import seaborn as sns\n",
    "\n",
    "#data augmentation   \n",
    "train_datagen = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(1, 1), shear=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_datagen = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) \n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='C:/MLDL/train_another', transform=train_datagen)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "validation_dataset = datasets.ImageFolder(root='C:/MLDL/validation_another', transform=val_datagen)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=20, shuffle=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42bb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "history = {'loss': [], 'val_loss': [], 'acc': [], 'val_acc': []}\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    for i, data in enumerate(tqdm(train_loader), 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.float().view(-1,1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracy += (torch.round(outputs) == labels.float().view(-1,1)).sum().item() / len(labels)\n",
    "        \n",
    "        if i % 20 == 19: # every 20 batches\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0.0\n",
    "                accuracy_val = 0\n",
    "                \n",
    "                start = time.perf_counter()\n",
    "                for val_data in validation_loader:\n",
    "                    val_inputs, val_labels = val_data\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss += criterion(val_outputs, val_labels.float().view(-1,1)).item()\n",
    "                    accuracy_val += (torch.round(val_outputs) == val_labels.float().view(-1,1)).sum().item() / len(val_labels)\n",
    "                val_loss /= len(validation_loader)\n",
    "                accuracy_val /= len(validation_loader)\n",
    "                history['loss'].append(running_loss / 100)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['acc'].append(accuracy / 100)\n",
    "                history['val_acc'].append(accuracy_val)\n",
    "                running_loss = 0.0\n",
    "                accuracy = 0.0\n",
    "    end = time.perf_counter()\n",
    "    print('Epoch {}/30, 10000/10000 [==============================] - time: {:.4f} ms/step - loss: {:.4f} - acc: {:.4f} - val_loss: {:.4f} - val_acc: {:.4f}'.format(\n",
    "        epoch + 1, end - start, history['loss'][-1], history['acc'][-1], history['val_loss'][-1], history['val_acc'][-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd206838",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72234725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model with transfer learning and data augmentation\n",
    "torch.save(model.state_dict(), 'tomnod_transfer_dataAugment.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the loss and accuracy\n",
    "\n",
    "\n",
    "plt.plot(epochs, history['acc'], 'bo', label='Training acc')\n",
    "plt.plot(epochs, history['val_acc'], 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy with data aug and dropout (Adam)')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "epochs = range(1, len(history['loss']) + 1)\n",
    "plt.plot(epochs, history['loss'], 'bo', label='Training loss')\n",
    "plt.plot(epochs, history['val_loss'], 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss with data aug and dropout (Adam)')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
