{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0de9b99",
   "metadata": {},
   "source": [
    "## Le deuxième objectif du projet consiste à expliciter et à implémenter 2 approches de transfer learning : \n",
    "**A : l’utilisation des features d’un DL avant la couche dense comme représentation des images puis apprentissage d’un modèle de ML “classique”**\n",
    "**B : fine-tuning d'un modèle existant à de nouvelles données**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092ce89",
   "metadata": {},
   "source": [
    "# Transfer learning de type A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e5e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changement de répertoire\n",
    "import os\n",
    "os.chdir(\"C:/MLDL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c44e7",
   "metadata": {},
   "source": [
    "## Avec Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad1d90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chrys\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\chrys\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRANSFER LEARNING\n",
    "#The authors did that : using feature extraction from ImageNet VGG16 - library : keras\n",
    "#We do : using feature extraction from ImageNet RESNet50 - library : pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "conv_base = models.resnet50(pretrained=True)\n",
    "layers = list(conv_base.children())[:-2] #on a un poids de 2048 \n",
    "conv_base = nn.Sequential(*layers) \n",
    "conv_base.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e002dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 1: Fast feature extraction without data augmentation\n",
    "#Run the conv_base on the dataset and save as Numpy array on disk\n",
    "#Then build the dense layer on this\n",
    "#This is faster to run, but we cannot augment the data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "train_dir = \"C:/MLDL/train_another\"\n",
    "validation_dir = \"C:/MLDL/validation_another\"\n",
    "test_dir = \"C:/MLDL/test\"\n",
    "#transform\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(150),\n",
    "    transforms.CenterCrop(150),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transform)\n",
    "validation_dataset = datasets.ImageFolder(validation_dir, transform=data_transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=data_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=20, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc696ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define extract_features\n",
    "#generators yield data indefinitely\n",
    "#have to break after we have seen every image once\n",
    "#try parallized (cuda)\n",
    "\n",
    "import torch.cuda\n",
    "\n",
    "\n",
    "def extract_features(directory, sample_count, batch_size):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    features = torch.zeros(size=(sample_count,2048,5,5), dtype=torch.float32).to(device)#on impute les poids de Resnet\n",
    "    labels = torch.zeros(size=(sample_count,), dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize((150,150)), #dimension de la cible\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    image_dataset = datasets.ImageFolder(root=directory, transform=data_transform)\n",
    "    data_loader = torch.utils.data.DataLoader(image_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in data_loader:\n",
    "        inputs_batch, labels_batch = inputs_batch.to(device), labels_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            features_batch = conv_base(inputs_batch)\n",
    "        features[i * batch_size: (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size: (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017f0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(train_dir, 10000, 20)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 2000, 20) #batchs(20) à remettre en pyTorch\n",
    "test_features, test_labels = extract_features(test_dir, 2000, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd353a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Etape de flatten\n",
    "train_features = train_features.reshape(10000, 2048*5*5)\n",
    "validation_features = validation_features.reshape(2000, 2048*5*5)\n",
    "test_features = test_features.reshape(2000, 2048*5*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a678ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#etape de tentatives divers et variées pour réussir à connecter ensuite la couche connectéee complètement\n",
    "import numpy as np\n",
    "train_features = np.reshape(train_features, (10000, 2048*5*5))\n",
    "validation_features = np.reshape(validation_features, (2000, 2048*5*5))\n",
    "test_features = np.reshape(test_features, (2000, 2048*5*5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70853161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Loss: 0.3796\n",
      "Epoch [20/30], Loss: 0.3474\n",
      "Epoch [30/30], Loss: 0.2918\n"
     ]
    }
   ],
   "source": [
    "#define the densely connected layer\n",
    "#import Optimizers\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2048*5*5, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'#pour activer la parallélisation\n",
    "model = DenseNet().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=2e-5)\n",
    "\n",
    "train_labels = train_labels.reshape(-1, 1).float()\n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    # Forward pass\n",
    "    outputs = model(train_features)\n",
    "    loss = criterion(outputs, train_labels)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "               .format(epoch+1, 30, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9551b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(x=range(30), y=loss_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14c2befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model with pretrained features in Numpy array (no data augmentation)\n",
    "torch.save(model.state_dict(), 'tomnod_transfer_RESNET.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad94f8f",
   "metadata": {},
   "source": [
    "# Transfer learning de type B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f1138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction with data augmentation and dropout\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, conv_base):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv_base = conv_base\n",
    "        self.fc1 = nn.Linear(150 * 150 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_base(x)\n",
    "        x = x.view(-1, 150 * 150 * 3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ba30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = ... # The pre-trained convolutional base\n",
    "model = ConvNet(conv_base)\n",
    "\n",
    "for param in model.conv_base.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "train_datagen = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(1, 1), shear=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "test_datagen = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='C:/MLDL/train_another', transform=train_datagen)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root='C:/MLDL/validation_another', transform=test_datagen)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=2e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "history = []\n",
    "for epoch in range(30):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss +="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72234725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model with transfer learning and data augmentation\n",
    "torch.save(model.state_dict(), 'tomnod_transfer_dataAugment.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402959a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss and accuracy for detection overfitting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#Get the training and validation accuracy and loss from the training history\n",
    "train_acc = history['train_accuracy'] \n",
    "val_acc = history['val_accuracy'] \n",
    "train_loss = history['train_loss'] \n",
    "val_loss = history['val_loss']\n",
    "#Get the number of epochs\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "#Plot the training and validation accuracy\n",
    "plt.plot(epochs, train_acc, 'bo', label='Training acc') \n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc') \n",
    "plt.title('Training and validation accuracy with pretrained features and data augmentation') \n",
    "plt.legend()\n",
    "plt.figure()\n",
    "#Plot the training and validation loss\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training loss') \n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss') \n",
    "plt.title('Training and validation accuracy with pretrained features and data augmentation') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0dae9",
   "metadata": {},
   "source": [
    "# Transfer learning de Type A - autres modèles de Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc57d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a logistic regression using Numpy array features (after passing through convultional based)\n",
    "#nous retrouvons les mêmes résultats que les auteurs\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# XGBoost\n",
    "xgb_pipeline = Pipeline([(\"xgb\", XGBClassifier(n_jobs=-1))])\n",
    "xgb_pipeline.fit(train_features, train_labels)\n",
    "xgb_score1 = xgb_pipeline.score(validation_features, validation_labels)\n",
    "print(\"Validation accuracy (XGBoost) = \", xgb_score1)\n",
    "xgb_score2 = xgb_pipeline.score(test_features, test_labels)\n",
    "print(\"Test accuracy (XGBoost) = \", xgb_score2)\n",
    "\n",
    "# Random Forest\n",
    "rf_pipeline = Pipeline([(\"rf\", RandomForestClassifier(n_jobs=-1))])\n",
    "rf_pipeline.fit(train_features, train_labels)\n",
    "rf_score1 = rf_pipeline.score(validation_features, validation_labels)\n",
    "print(\"Validation accuracy (Random Forest) = \", rf_score1)\n",
    "rf_score2 = rf_pipeline.score(test_features, test_labels)\n",
    "print(\"Test accuracy (Random Forest) = \", rf_score2)\n",
    "\n",
    "# LightGBM\n",
    "lgbm_pipeline = Pipeline([(\"lgbm\", lgb.LGBMClassifier())])\n",
    "lgbm_pipeline.fit(train_features, train_labels)\n",
    "lgbm_score1 = lgbm_pipeline.score(validation_features, validation_labels)\n",
    "print(\"Validation accuracy (LightGBM) = \", lgbm_score1)\n",
    "lgbm_score2 = lgbm_pipeline.score(test_features, test_labels)\n",
    "print(\"Test accuracy (LightGBM) = \", lgbm_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2743d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('tomnod_2_100epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0bbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                    \"C:/MLDL/test_another\", #change this for balanced/unbalanced test set \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary')\n",
    "y_pred = []\n",
    "y_label = []\n",
    "for i in range(450):\n",
    "    x,y = test_generator.next()\n",
    "    temp_y = model.predict(x)\n",
    "    y.tolist()\n",
    "    temp_y.tolist()\n",
    "    y_pred.extend(temp_y)\n",
    "    #print(temp_y)\n",
    "    #print(y)\n",
    "    #print('')\n",
    "    y_label.extend(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc7493",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59231885",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_label, y_pred,pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed943683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03be6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc_keras)\n",
    "#les auteurs ont trouvé 0.9963734999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='AUC area = {:.3f}'.format(auc_keras))\n",
    "#plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve of the best model with unbalanced test set')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafcfab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see which image is misclassified\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                    \"C:/MLDL/test\", #change this for balanced/unbalanced test set \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary')\n",
    "y_pred = []\n",
    "y_label = []\n",
    "for i in range(50):\n",
    "    x,y = test_generator.next()\n",
    "    y = y.ravel()\n",
    "    temp_y = model.predict(x)\n",
    "    temp_y = np.round(temp_y.ravel())\n",
    "    misclassification = np.absolute(y-temp_y)\n",
    "\n",
    "    #show the misclassification\n",
    "    misclass_index = np.where(misclassification > 0.5)\n",
    "    for j in range(len(misclass_index[0])):\n",
    "        plt.figure()\n",
    "        #print(misclass_index[0][j])\n",
    "        plt.title('label is {}, prediction is {}'.format(y[misclass_index[0][j]],temp_y[misclass_index[0][j]]))\n",
    "        plt.imshow(img_to_array(x[misclass_index[0][j]]))\n",
    "    misclass_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b7c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see how the generator encodes the images into binary or multiclass:\n",
    "test_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb887e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
